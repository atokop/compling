
Babbage


tag:www.economist.com,2009:21005042
2011-09-06T00:50:36+00:00
Drupal Views Atom Module

Will the internet burst into tiers?

tag:www.economist.com,21527059
2011-09-05T15:40:35+00:00
2011-09-05T15:40:35+00:00

   Tiered traffic could boost providers' profits while charging consumers fairer rates  

G.F. | SEATTLE
http://www.economist.com


    EACH year billions of billions of bytes of data flit across the internet. Ensuring that things do not get clogged up are transit providers, firms whose job it is to link up internet service providers which, in turn, offer internet access to individuals, businesses, government entities, or anyone else who wishes to tap into the web. As overall capacity increases, however, prices transit providers can charge drop precipitously. Yet they could be earning higher margins even as customers are priced at fairer rates. That, in any case, is the conclusion of a recent paper by Nick Feamster, of the Georgia Institute of Technology, and his colleagues. Dr Feamster notes that traffic is nearly always priced at blended rate. This resembles an indiscriminate road tax where all car owners pay the same lump sum regardless of how much they actually drive. The researchers therefore decided to compare the current pricing model with alternatives where the longer the distance a bit had to travel between two routing centres, the more cost they ascribed to its journey.In one version, with an unlimited number of tiers, every packet of data was priced individually based on the actual distance travelled, much like a metered cab fare. Another divvied up the packets into three or four discrete bands, such as local, regional and international, for instance, akin to public-transport where a price of a single ticket depends on how many zones one crosses to get to the destination.Such tiered strategies should, Dr Feamster reckons, plump up transit providers' margins. Higher demand in cheaper tiers should offset the cuts in fares there, while usage across the more expensive bands, which anyway make up a smaller proportion of traffic, tends to be less price sensitive. As for consumers, a content provider that mostly exchanges data within a single country, for instance, could make substantial savings if it were spared having to subsidise firms that use more expensive international link-ups. An infinite number of tiers would, of course, be the most economically efficient. However, it imposes its own costs, such as highly complicated billing. With three or four tiers, transit providers' margins would be only slightly lower (and, conversely, consumers would, on average, get a minimally better deal) than if they charged each packet for the precise distance travelled. But the tiered system would be much cheaper to manage. So far the researchers only looked at cost as a function of distance. Dr Feamster says the next step is to examine models in which price is tied to the time it took for a packet to be delivered, time of day or a slew of other parameters. Ultimately, dynamic pricing, which directly tracks demand as measured by network load, say, could be considered. All this may concern parts of the internet with which few people are familiar. Which is not to say it would not have palpable consequences for anyone cruising along the infobahn.]]>  


Duly notarised

tag:www.economist.com,21528172
2011-09-04T13:30:22+00:00
2011-09-04T13:30:22+00:00

   Distributed trust is beginning to replace blind faith in secure communications over the internet  

G.F. | SEATTLE
http://www.economist.com


FOR all its decentralised charm, the internet remains a top-down affair when it comes to security. Every time you connect to a secure website it is parties anointed with authority from on high that tell you whether or not the site should be trusted. Such dogma has been in place since the dawn of secured web communications. But heretics are becoming increasingly vocal. "It is insane to me that we can pick an organisation or set of organisations that we can trust not just now, but for ever, whether they continue to behave appropriately or not," laments Moxie Marlinspike, the man behind one of four related reformation movements which are beginning to challenge the old order. As Babbage discussed in an earlier post, part of the bedrock of internet security are digital documents called certificates. These are bundles of cryptographic information issued by third parties known as certificate authorities (CAs). A cryptographic watermark ensures that the certificate was in fact issued by the CA featuring on it. Certificates themselves do not guarantee that the website has been well set up and secured. (That falls to other parties like TRUSTe that perform certain forms of routine audits and offers seals of approval.) Rather, they are bound together cryptographically to particular internet domains to assure the user that he is in fact connected to the desired site, and not a malicious one pretending to be it. The point is to prevent so-called man-in-the-middle attacks, in which an interloper sneaks in between two parties and relays their mutual messages. Browsers and operating systems use built-in lists of trusted CAs. However, if a CA's reliability is called into question, the lists, which contain hundreds of names, cannot be easily updated. On August 29th news broke that a Dutch CA, DigiNotar, had improperly issued a certificate for all Google domains to a party other than the search giant. It ought not have issued such a certificate at all, and certainly not to a different firm. The company says its systems were subverted and independent security observers say as many as 250 certificates for an unknown number of domains were released. The Google document apparently remained valid for five weeks.When a purported Iranian user spotted the certificate in his country pundits noted that repressive governments like Iran's could use it to spy on their citizens. It would be especially useful to eavesdroppers capable of re-routing internet traffic and poisoning domain-name lookups, where a domain name typed in by a user is turned into an numeric machine address, but not the one the user intended. A user in such a subverted system might click on https://mail.google.com, the secure address for Gmail, say, only to be redirected to a computer operated by someone else. Normally, if the fake site then presented a certificate from an unknown or untrusted source, the browser would flag this up as a security threat. If, however, the fake certificate came with the imprimatur of a trusted CA, the user would be none the wiser.DigiNotar's illegitimate certificates have now been revoked and blacklisted. This alone does not, however, make the problem go away. Although browsers are programmed to consult such blacklists, whenever access to a list is blocked—something governments, for instance, can do all too easily—a browser will not signal this to the user. This is because the blacklist services are often unreliable in returning results: browsers would constantly warn that they could not assure the integrity of any secure site, not just those which have had their certificates revoked. And in any case, certificates are rarely revoked so the whole exercise is bereft of sense.For their part, browser makers rushed to release full updates to their software that remove DigiNotar from the built-in list of trusted CAs; some browsers now also have an explicit list that blocks these and other certificates permanently. Users who do not download these updates (or fail to make tedious manual tweaks to their existing browsers) run the risk of accepting the dodgy certificates, thereby compromising their communications. The kerfuffle has led to renewed calls to overhaul the internet's existing security architecture by dispensing with CAs' anointed role  altogether, or at least making it a complement to a suite of validation methods. The idea was first mooted in 2008 by researchers at Carnegie-Mellon University in Pittsburgh. That led to their Perspectives Project. The three other efforts, by Mr Marlinspike (Convergence), the Electronic Frontier Foundation (SSL Observatory) and Google (Certificate Catalog), all acknowledge Perspectives' leading role. In the Perspectives system CAs could still issue certificates. However, there would be no requirement to have a list of permanently trusted issuers baked into browser software. Instead, every time a browser retrieved a certificate from a secure website, it would consult an array of "notary" servers. The job of these would be to perform regular and exhaustive scans of all the secure servers on the internet from different parts of the globe, retrieving all their current certificates and recording the data in an abstracted shorthand called a fingerprint. If over time a certificate remains identical that gives some confidence that it is legitimately associated with a given domain. When a user's browser connects to a secure server it retrieves the associated certificate and compares it to the notary records. If the two match, the site is probably kosher. If not, the browser sounds an alarm.Crucially, no one is being forced to use Perspectives' notary servers. In principle, any party may run such servers, and the code is available to install or develop notary servers.  The Perspectives group expects trusted not-for-profit organisations and corporations might one day offer notary services to the public. Some companies and governments may, of course, wish to set up their own notaries and trust only them. Mr Marlinspike has taken a slightly different tack. His notary software allows for multifarious forms of validation. Besides a system like Perspectives' users or groups may, for instance, wish to examine the route data took from a server to a browser and compare that against a constantly updated list that is publicly distributed to internet routers. If the server is located on a different network than where it is expected to be found, it's suspicious. Another technique would rely on checking certificate fingerprints that have been inserted into domain-name-system records (described in the earlier post). Both Perspectives and Convergence offer a Firefox add-on.Google, meanwhile, has incorporated the Certificate Catalog into its Chrome browser, the first such direct integration. (It was Chrome that alerted the ostensible Iranian user to DigiNotar's suspect certificate.) As for the Electronic Frontier Foundation, whose mission is to support digital privacy rights and transparency on the internet, its goal is purely informative for now. The technical advantage of notary approaches is that no website or other secure server needs to amend their configuration or install upgraded software. Makers of browsers and operating-systems could add support for dynamic certificate validation to replace the frozen CA lists that are at the root of the current troubles. In the meantime, users can install security-enhancing plug-ins.]]>  


Forking Android

tag:www.economist.com,21528462
2011-09-03T22:43:04+00:00
2011-09-03T22:43:04+00:00

   Amazon's rumoured tablet might upset Apple's cart  

G.F. | SEATTLE
http://www.economist.com


    AMAZON twice upset the book industry's apple cart in recent years. When it first burst onto the the scene in 1995, the virtual bookshop let readers order pretty much any book they wanted, without getting out of the house. Since it dispensed with the need for expensive brick-and-mortar outlets it could offer reads at heavily discounted prices. Then, even more disruptively, Amazon launched the Kindle, the first widely sold electronic-book reader, transforming a trade which has, since the invention of the printing press in the 15th century, relied on paper.Rumours have been rife for some time that Amazon is taking aim at another cart, this one dominated, rather fittingly, by Apple. The online retailer, which has over the years diversified away from books into just about every conceivable consumer product, was thought to be preparing to launch a general-purpose tablet running a form of Google's Android operating system. Now TechCrunch, a technology website, has offered a detailed overview of the next Amazon Kindle, a 7-inch tablet that it says is in final production testing and will hit the virtual shelves in October, just in time for Christmas, with a price tag of $250. TechCrunch, which is sometimes criticised for sketchy information, may have got some of the details wrong, but the gist of its apparently hands-on account rings true.So far alternatives to Apple's svelte tablet have failed to inspire. Apple still controls about two-thirds of the market for such devices. Some rivals, like HP, which announced it would stop making its underwhelming TouchPad, have thrown in the towel. Apple's strength stems from several factors. Since the firm uses the same operating system across all of its mobile devices, most of the existing library of hundreds of thousands of apps developed for the iPhone were available for the iPad, too. It also made it easy for developers to adapt existing apps to the iPad's larger screen. Some tens of thousands of apps now work exclusively on the iPad or in dual small- and large-screen versions. The company's cash hoard has apparently allowed it to purchase components, including 10-inch touchscreens, in such quantities that it obtains a higher margin, while locking down supplies. And its swanky Apple Stores let consumers easily try out and purchase the gadgets—or dispatch them for repair. Less frequently mentioned, though no less important, is Apple's vast catalogue of television programmes and films, and the infrastructure for delivering them. (Nearly all downloadable music stores offer only unprotected and standardised MP3 or AAC formatted audio, which can be played on any device.) Google, HP, RIM and other firms often stress how delightful watching video is on their assorted tablets. None, however, has offered seamless access to such content. Streaming was possible using an app from Netflix on a limited number of platforms and phone models, but streaming video, tolerable over Wi-Fi, is difficult and expensive over mobile networks. For travel, downloading remains a must.  Unlike earlier challengers to Apple, Amazon already has deals in place to stream tens of thousands of films and TV shows. It once offered downloads to certain devices and could easily revisit such arrangements. Film studios and television networks ought to embrace a rival to Apple which, as the sole serious incumbent, has exerted pressure on them to forge deals and cut prices. The operating system may also prove less of a challenge for Amazon than it did for other tablet hopefuls. HP bought Palm for its webOS system, and RIM grabbed QNX, another OS maker, to power its PlayBook. Like Motorola (which is being acquired by Google) and Samsung, Amazon is sticking with Google's Android, broadly similar to a version of Linux it has used for years to run its e-book reader. But if the TechCrunch report is accurate, Amazon has decided to ditch its own Linux flavour, and take a mature, licence-free version of the Android open-source software and start building an in-house system on top of it. Such forking, as the practice is known in techie circles, means that development paths of the open-source software and Amazon's version of it will diverge. As a result, software built for one will not easily run on the other. This would let Amazon create a platform independent of Google, which remains Android's main developer. The open-source code may be licence-free, but the Android ecosystem is constrained. Google only permits other firms to use the Android trademark, the associated app store, Google-created apps and any related data if their versions of the software pass certification tests and adhere to other terms.Amazon had already annoyed Google by launching an Android app store without the search giant's consent. So far it only offers thousands of apps whereas Apple's App Store and the official Android Market offer hundreds of thousands (though only several hundred of the Android apps work on tablets; the rest are meant for smartphones). But it was an important precedent. If Amazon sells enough devices which run existing Android apps, developers may chose to move along with it—and away from the ebbing Android tablet mainstream. Like Apple, Amazon has tens of millions of user accounts already linked to credit cards, and a reputation for no-fuss customer service. Forrester, a research firm, thinks that Amazon could shift as many as 5m devices by the end of the year, if the price is right. That would still put it behind Apple's 9.25m iPads sold in its most recent fiscal quarter and over 20m in the last fiscal year. But it would be well placed to chase after Apple's cart and, one day, perhaps even to upset it.]]>  


Endless work

tag:www.economist.com,21528177
2011-09-03T15:57:08+00:00
2011-09-03T15:57:08+00:00

   Business travellers use mobile phones to heap more work into their already busy lives, but occasionally flip the off switch  

G.F. | SEATTLE
http://www.economist.com


    PERUSING the latest report on the habits of frequent business travellers, published by iPass, an outfit that sells roaming internet access to firms, Babbage could not help but feel for the modern white-collar worker. An earlier study by iPass found that majority of highly mobile workers around the world already toil for more than 50 hours in an average week, with 20% working 60 hours or more. According to the latest survey, three-quarters of the 3,100 businessmen and women interviewed say they work five to 20 hours a week more than their official working hours suggest. Downtime is becoming scarcer, with laptops and smartphones using ubiquitous wireless broadband connections, opening up the possibility to work anywhere and at any time. Expanding airborne internet access means that workers can be tapped even on traditionally tranquil flights.Human-resource managers would understandably rejoice at what amounts to more uncompensated overtime. More surprisingly, the respondents themselves seem remarkably cheery about the ever longer hours. They are also unfazed by the erosion of the divide between work and personal life. All but 6% believe that they are at least as productive with the flexibility to work nearly anywhere; over half say they are substantially more so. At the same time, nearly two-thirds said they found it easier to balance work and personal commitments, and about one in two were "marginally" or "substantially" more relaxed. This odd combination of more working hours and less down time with higher performance and greater satisfaction might be explained by another trend. In 2010, just 47% of respondents admitted to completely disconnecting from technology while awake. In 2011 that number soared to 68%. In other words, employees may have less and less time for personal life. But they are learning to take fuller advantage of the little they do have.]]>  


And now, the gadget forecast

tag:www.economist.com,21527004
2011-09-03T12:17:02+00:00
2011-09-03T12:17:02+00:00

   A site predicts price movements for existing gadgets and launch dates for new ones  

G.F. | SEATTLE
http://www.economist.com


    ASK any regular traveller who pays for his own tickets about fare pricing and you are likely to hear a string of obscenities. The variability from week to week—sometimes minute to minute—in the cost of a flight from point A to B can be maddening. Airlines use all the computational power at their disposal to maximise their returns by setting fares based on current and predicted demand. As a result, just poking around on airline and aggregator sites may alter the price. Naturally, it did not take long for a response to emerge. Sites like Farecast, Kayak and others use historical pricing information, among other things, to predict, with varying degrees of confidence, whether a rate currently on offer was likely to rise or fall. These data boost the buyer's confidence that he is not a sucker if he clicks the "pay" button now. Far less understood is how rapidly prices for consumer good change over both short and long periods, says Oren Etzioni, the co-founder of Farecast (since sold to Microsoft, and called Bing Travel). Eyeing an opportunity, Mr Etzioni launched Decide. The company, which is based in Seattle, estimates when a successor to a specific gadget will be rolled out and the odds that the lowest purchase price anywhere online will go up or down in the near future.  Dr Etzioni, a computer scientist at the University of Washington in Seattle who has founded four firms in all, says Decide relies on three main data sources: pricing data, news and rumours, and technical specifications. Pricing data comes from a variety of sources. Most are the company's trade secret, though they always include current prices of goods and sales data. The model also uses feedback about how its predictions fare over time to fine-tune their probability estimates. With news and chatter, Decide scores sites by how accurate their scoops are for particular categories of goods. The algorithm discounts rumour-mongers and gives a greater weight to reliable sources. So far, the firm has amassed a year's worth of data, many thousands of gigabytes in total. These reveal unexpected consumer behaviour. For example, when a new model of a well-reviewed product is released, the older model's price paradoxically tends to go up and people continue to purchase it. Dr Etzioni explains that this may be because a new model has fewer reviews, which can make the older model seem better, and newer models often have a cheaper list price than an older one. This leads  buyers to suspect the newer item is of lower quality (which can be true, but does not have to be). At the same time stocks of older products dwindle putting upward pressure on the price.The Decide model differs substantially from simple price comparison services. Such sites, which date back to the 1990s, aggregate prices from many online retailers. Decide presents a similar list to show current prices, but also attempts to advise consumers on the most opportune moment to make the purchase. In time this might force sellers to slash their margins and discount obsolete products languishing in inventories as buyers await the arrival of new versions. That said, retailers' prices are already at or near equilibrium. At least half the time, Dr Etzioni says, Decide tells consumers that it is a good time to buy, as prices are stable and there is no replacement in the offing. (The exact numbers depend on product category and time of year.)  Your correspondent's grandfather taught him always to haggle. Haggling empowers the buyer, affording a sense that he holds some direct sway over the price of a desired good. Modern consumer societies tend to rely on mechanisms of price discovery which, though economically efficient, are less emotionally gratifying. Auctions offer a degree of explicit control but are often impractical. By giving advice on not just where, but when to buy, Decide helps make negotiating the online bazaar a slightly less passive experience.]]>  


Difference Engine: Whole lot of shaking

tag:www.economist.com,21528148
2011-09-02T07:19:00+00:00
2011-09-02T07:19:00+00:00

   Small earthquake in Virginia rattles more than nerves  

N.V. | LOS ANGELES
http://www.economist.com


DRENCHED and battered by Hurricane Irene, and facing a clean-up bill pushing $10 billion, residents on the east coast of America have understandably had more on their minds over the past week than the earthquake which struck the Piedmont region of Virginia a day before the tropical storm swept ashore. Yet, the shaking caused by so modest a tremor, at such distances from the epicentre, caught experts by surprise. In the long term, the Virginian earthquake could trigger a bigger shake-up in disaster precautions at nuclear-power stations in America than even the Japanese catastrophe at Fukushima.The magnitude 5.8 quake that struck 38 miles (61km) north-west of Richmond was felt as far west as Wisconsin, as far south as Atlanta, Georgia, and as far north as Montreal, Canada. Damage was reported over 300 miles away in Brooklyn, New York. The White House, the Capitol and other buildings in Washington, DC, had to be evacuated. Cracks were even detected in the Washington Monument—the tallest stone building in the world—which is now closed indefinitely. Washington National Cathedral lost capstones from three of its spires, and cracks were found in several of its flying buttresses. All this from a seismic event that would barely rate as an after-shock in California. Earthquakes on the West Coast are more frequent and can pack a much greater punch. Size for size, though, their rattlings are rarely felt at quite such distances.Put that down to the difference in the age of the rocks. As the relatively young Pacific plate dives beneath the continental land mass, sudden slippages along the grinding rock faces breed swarms of earthquakes, big and small. But the majority of shockwaves so created quickly dissipate as they run into fractures and hotter rocks deep beneath the surface. Subjected to stress, rocks above 300ºC or so tend to flow rather than rupture. And because fluids cannot handle shear forces anywhere near as well as solids, the potent S-waves from an earthquake (the secondary, or shear, waves that shake the ground from side to side and knock down buildings in the process) eventually fizzle out. An earthquake’s faster-moving P-waves (primary, or pressure, waves that push the ground longitudinally) get through, but they carry far less energy and do little damage.By contrast, the crustal rocks that created the Appalachian and Allegheny mountains in the east of the country, being hundreds of millions of years older, have had ample time to cool down. In the process, they have become denser and harder. Unlike in California, seismic activity on the East Coast is usually shallow and well away from the boundaries where tectonic plates collide. As a result, earthquakes in bedrock east of the Appalachians tend to ring the earth like a steel girder being struck with a hammer. West of the Rockies, the effect is more like a rubber tyre bouncing over a pothole. All told, eastern earthquakes can shake areas ten times greater than comparable western ones. And they do so at much higher frequencies. That makes a big difference to the kind of damage done. The lower-frequency vibrations in California cause greater damage to large, rigid structures such as office blocks, bridges and elevated highways. They might even rent a nuclear reactor’s containment vessel were a major seismic thrust to occur on a nearby fault. One of the two nuclear power stations in California, at Diablo Canyon, near San Luis Obispo, is only a few hundred yards from an active fault which, coupled with an even bigger one three miles away, has the potential to produce a magnitude 7.3 earthquake. Recall that the quake which jolted Japan’s (and the world’s) largest nuclear-power station, at Kashiwazaki-Kariwa on the Japan Sea coast, in 2007, was a more modest magnitude 6.8 (see “Shaken, but not stirred”, August 10th 2007). The damage there was such that three of the station’s reactors remain shut to this day. Until the Fukushima disaster, Kashiwazaki-Kariwa was Japan's biggest nuclear accident.By contrast, the higher-frequency shaking that takes place east of the Appalachians implies longer-traveling shockwaves of shorter wavelength. While such shocks are unlikely to topple multistory parking structures, flatten apartment blocks or toss freeways in the air (as happened in Los Angeles during the magnitude 6.7 Northridge earthquake in 1994), they can play unexpected havoc with delicate instruments and electronic devices.This explains why the pair of reactors at the North Anna nuclear-power station, ten or so miles from the epicentre of last week’s earthquake in Virginia, were knocked offline. Sensitive relay switches, used to protect the plant’s transformers, seem to have interpreted the earthquake’s high-frequency shaking as an electrical spike coming down the line, and switched off the power supplied from the grid for running the plant. As a result, the reactors had to be immediately “scrammed” (ie, shut down rapidly by ramming neutron-absorbing control rods into the core to kill the nuclear reaction) and emergency generators fired up to provide electricity for cooling pumps and other safety equipment. One of the diesel generators malfunctioned—a disturbingly common problem, apparently, at nuclear power stations, and one of the key underlying reasons for the disaster at Fukushima. Fortunately, the remaining three standby generators functioned properly. No damage to the plant was reported, other than cracks in ceramic insulators on one of the transformers. Even so, the Virginia incident has triggered calls for fresh scrutiny of the dozens of ageing nuclear reactors in the United States. The majority were constructed to standards designed to survive conditions more common in the west of the country—despite the fact that only eight of the 104 remaining reactors in America are west of the Rockies. Most were also built before the oil industry’s modern 3D seismic techniques started discovering active faults no-one previously knew existed. The day after the Virginian earthquake, Edward Markey, a Democratic congressman whose district includes the Massachusetts Institute of Technology and Harvard University, as well as a big chunk of the East Coast’s high-tech industry, wrote to ask the Nuclear Regulatory Commission (NRC) whether the earthquake had exceeded the seismic safety specification the North Anna plant had been built to. Mr Markey also urged the NRC to embrace the recommendations of its own “Near-Term Task Force" on Fukushima, which the NRC has continued to drag its feet on. In a recent report (“Fukushima Fallout”) produced by his own office, Mr Markey noted that the NRC had failed to incorporate its technical staff’s recommendations, despite the fact that new information indicates a much higher probability of core damage caused by earthquakes than previously thought. Based on seismic data from 1989, the NRC expects the number of events causing damage to a reactor’s core to be an incredibly minuscule 0.0000038 per year—equivalent to a reactor failing, on average, once every 260,000 years. However, reworking the numbers using seismic data from 2008, and computing the risk for the whole fleet of reactors in America being operated for the further 20-year extension being sought for their current licences, Mr Markey’s staff expect the risk to increase 7,000-fold to a probability of 0.026 per year—ie, one nuclear disaster somewhere in the country every 38 years. In short, another Three-Mile Island some time between now and 2049. If that is indeed the case, it is surely time for the NRC to embrace, rather than resist, the lessons of Fukushima in general, and North Anna in particular. Rightly or wrongly, NRC’s foot-dragging on regulatory reform has given the impression of favouring industry interests over public safety. That is neither in the best interest of the nuclear industry itself, nor the public’s need for cheap, carbon-free power.]]>  


To boldly go where no start-up has gone before

tag:www.economist.com,21528165
2011-09-01T08:59:01+00:00
2011-09-01T08:59:01+00:00

   A course in changing the world produces its third batch of graduates  

L.B. | SAN FRANCISCO
http://www.economist.com


THE motto of the Starfleet Academy, the nursery of future leaders and humanitarians in the fictional universe of Star Trek, was "Ex astris, scientia". Peter Diamandis, an entrepreneur who set up the X-Prize Foundation to spur innovators into tackling grand technological challenges, has turned the credo around. He wants to use knowledge to reach for the stars—both literally, by promoting private human spaceflight, a cause particularly dear to Mr Diamandis, and figuratively. Together with fellow techno-utopian Ray Kurzweil, he created what he calls “a Starfleet Academy for the world's biggest challenges”. Founded in 2009, Singularity University is based at NASA's Ames Research Center in Silicon Valley and inspired by Mr Kurzweil's idea of technological singularity, an innovation (like artificial superintelligence) which will one day completely upend the way the world works. Its ten-week graduate program attracts serial entrepreneurs, engineers, fighter pilots, roboticists and political advisers. Their rather ambitious study goal is to get an inkling of the most rapidly-advancing technologies, and then to figure out how they can help change the world—or, more specifically, improve the lives of one billion people in the next decade.Star faculty members include Astro Teller, the head of innovation at Google, and Vint Cerf, one of the fathers of the internet. They are joined by a bevy of experts in what Mr Diamandis refers to as “exponentially growing technologies”: nanotechnology, artificial intelligence, brain-machine interfaces, synthetic biology and the like. Field trips take students to see the Valley's hottest new thing. Itineraries have included the headquarters of Tesla Motors, a maker of electric sports cars, Willow Garage, a lab which develops personal robotics, 3D printing centers, or the home of Google’s self-driving car. It is, in effect, a summer camp for geeks, albeit ones who will not blink when they tell you they intend to change the world.On August 25th the third graduating class presented ten business proposals to a crowd of their professors, angel investors and tech grandees, a couple of hundred in all. Each presentation lasted ten minutes, was long on ambition and excruciatingly short on detail—hardly surprising given that the teams had just five weeks to come up with their ideas. One of the most eye-catching projects was also the furthest along in its development. The pitch featured a video of a small robotic “quadrocopter”, hovering gracefully to an astral soundtrack. The goal of the company, called Matternet, is to use such robots to transport drugs and medical testing kits in regions without reliable roads. The current prototype is capable of carrying 2kg over a distance of 10km; the team envisions networks of ground stations, where the devices  could recharge on their way to more inaccessible destinations. The government of the Dominican Republic has already agreed to work with Matternet for a field test in that country. Matternet's ultimate goal is to build an unmanned aerial vehicles  capable of carrying not just medicines but people between villages; in effect a pilotless electric helicopter.Another initiative, called AstroTrash, is aiming even higher. It hopes to use Earth-based lasers to clear  space junk out of the path of satellites. The proposers, who includes Franz Gayl, a civilian science-and-technology adviser to the Pentagon, explains that the lasers would slow the debris down to the point where they fall out of lower-Earth orbit and vaporise in the Earth’s atmosphere on re-entry. The idea is not entirely novel; it was aired in March by NASA scientists. And it faces formidable  technological hurdles—it is far from clear whether lasers can actually pass through the Earth’s atmosphere retaining enough oomph to nudge orbital flotsam in any meaningful way. Perhaps even more daunting is the prickly diplomatic issue of the device's potential use in space warfare.More down-to-Earth projects included Corruption Tracker, a mobile platform for reporting corruption, Ignisolar, which already managed to construct a prototype of a cheap, flexible and lightweight solar panel, and Senstore, which helps the developer community build and commercialise prototype health sensors.All teams hope to turn their projects into start-ups. In most cases, though, only a few members will continue working on the ideas full-time; the rest have jobs or PhDs to get back to. That does not mean that the projects will fizzle—at least if the first two classes, no less vaultingly ambitious than the current crop, are any guide. Many projects have faded from view but a handful is slowly coming to fruition. Several start-ups are already up and running and a few of those have secured seed financing. One company which recently found investors, BioMine, exploits existing mining technologies to harvest valuable metals from electronic waste; another, H2020, received cash from the Chilean government which wants to use their mobile platform to crowdsource information on the availability and quality of water in slums.But perhaps the biggest success so far, judging by the excited chatter of this year's hopefuls, is a company called GetAround. It is a peer-to-peer car-rental service borne out of the academy's inaugural class. Its big idea is to get people to give up the cherished idea of owning a car—and embrace a future of autonomous, zero-emissions vehicles that will be summoned and driven by the iPhones of the future. Given the deep attachment many citizens of rich and, increasingly, developing countries feel to automobiles, the project's aim is indeed ambitious, though it remains unclear how it would dramatically improve the lot of one billion people. The same could be said for most other proposals. Ultimately, however, only time will tell whether Mr Diamandis's version of the Starfleet Academy will yield results any less fictional than its cinematic precursor.Correction: An earlier version of this blog post wrongly suggested that Peter Diamandis is a billionaire. This has been corrected.]]>  


Howzat!

tag:www.economist.com,21528164
2011-08-31T18:06:12+00:00
2011-08-31T18:06:12+00:00

   Technology is creeping into cricket—and addressing bowlers' perennial grievances  

A.A.K. | MUMBAI
http://www.economist.com


AS A fast bowler hurls the ball along the so-called "corridor of uncertainty" cricket fans hold their breath. When the orb lands in that area most batsmen struggle to tell whether to lean ahead, play back, or poke at it at all. A split-second of indecision is sometimes enough for the cherry to brush the outside edge of the bat on its way into the wicketkeeper’s gloves. However, faint nicks sometimes go unnoticed by the umpire. This, combined with the game's laws which stipulate that any doubt should be interpreted in the batsman's favour, leads many a bowler to feel put upon. Technology might offer them some solace.Unlike that other great British game, football, cricket has not shied away from technological novelties. On May 18th 1994, during a five day Test match (a format beloved of purists) between India and South Africa, Sachin Tendulkar, hailed by many as the greatest batsman of his generation, became the first cricketer to fall to an umpiring decision aided by slow-motion television replays. He was declared run out when he failed in time to ground his bat behind the crease, a white line in front of the stumps.Spotting run outs in this manner is relatively easy. Detecting nicks, by contrast, is tricky even in slow motion. So another newish technology, called the "Hot Spot", is also being deployed. It harnesses both slow-motion replays and infrared imaging. Heat-sensing cameras are pointed at the batsman from the boundary line. When the ball hits the bat, the batsman’s leg pad or the pitch, the resulting friction produces heat, which shows up in the cameras as a bright white mark. Hot Spot has become popular with umpires, as it makes it easier to deal with the huge number of appeals from the fielding side whenever the ball hurtles past the bat at speed. Warren Brennan, CEO of BBG Sports, the Australian company behind the Hot Spot, admits that the technology is imperfect. Where the afternoon sun is low, sunlight reflected off the bat may confuse the heat sensors. Here, using four, rather than the usual two cameras, helps, offering additional crosschecks. Renting four cameras cost around $10,000 a day, as opposed to $6,000 for just two, but that is not beyond the means of organisers or sponsors, especially of big international matches. Moreover, fielding players sometimes obstruct the camera's view of the batsman. By December, though, when India is sheduled to tour Australia, Mr Brennan hopes to mount the cameras on a trolley, so they could always shuffle to a favourable vantage point.A bigger problem is that a quick flourish of the bat sometimes appears blurred in the grainy black-and-white image (see picture). As a result, it can be difficult to tell whether the bright mark is actually on the bat—ie, the result of the ball nicking it—or was produced by some other heat-emitting event nearby. Plastic stickers on the bat’s edge are another concern. A sponsor’s logo generally covers the top quarter of a bat. On some occasions, though, both edges of the bat are coated. Since the stickers are often darker than the rest of the bat, they absorb more heat, and so appear brighter in the infrared footage. In the event of a subtle nick, the white mark may be impossible to discern against the background.In such cases, a different approach may be needed to dispel doubts. The Snickometer, or Snicko for short, is an audio receptor which listens for any variation in the sound as the ball whizzes past the bat. If the two touch, Snicko ought to pick it up. Unfortunately, it picks up any noise around the stumps, leading to too many false positives to be a reliable umpiring tool. It also takes a while to produce its analysis—too long, in the view of many, for the pace of the game to be maintained. As a result, it is employed mainly by commentators. Used in conjunction, however, the Snickometer and the Hot Spot could produce far more accurate results. Sensing an opportunity, Mr Brennan is already working on a hybrid Hot-Snick. Then there are other modes of dismissal, which call for different methods. Take "leg before wicket", when a batsman is given out if the ball strikes his pad where it would otherwise have gone on to hit one of the three stumps. Cricket pitches at different venues are characterised by different bounce, so a lot depends on how well an umpire is acquainted with the wicket. To help him, tracking systems like Hawk Eye (developed for cricket and now also used in tennis to determine line calls) employ complex algorithms to extrapolate the ball's most probable path. (The leg-before-wicket law also states that the batsman cannot be declared out if the ball touches the bat before hitting the pad, so many tracking systems incorporate Hot Spot to avoid mistakes.)"I don’t think this technology will ever be perfect," sighs Mr Brennan, "but we’ll keep trying." Bowlers are bound to embrace the innovations, as are umpires. Batsmen, on the other hand, might be forgiven a dash of Luddite indignation.]]>  


Beyond the Kindle

tag:www.economist.com,21528199
2011-08-31T15:19:45+00:00
2011-08-31T15:19:45+00:00

   Linux celebrates its 20th anniversary, Amazon looks to the tablet market and Google gets caught in a scandal over pharmaceuticals  

The Economist online
http://www.economist.com


    Linux celebrates its 20th anniversary, Amazon looks to the tablet market and Google gets caught in a scandal over pharmaceuticals]]>  


Comments not disabled on this post

tag:www.economist.com,21526575
2011-08-31T13:17:34+00:00
2011-08-31T13:17:34+00:00

   A frequently visited website chooses pseudonymity over anonymity  

G.F. | SEATTLE
http://www.economist.com


IN THE recent debate over whether every internet user should be somehow required, possibly by law, to identify himself by a real name, the popular blog site BoingBoing would have been expected to adopt a firm stance. Its editors and guest contributors—of which this Babbage is one—tend to be fierce defenders of digital freedoms and online privacy. Surely, then, the Directory of Wonderful Things, as BoingBoing likes to call itself, embraced perfect anonymity when it recently migrated its commenting system to a new software platform? Not at all. The BoingBoingers may be idealistic, but they also are practical. The site still requires users wanting to post comments to confirm registration by e-mail. Editors and moderators briskly remove and bar posters violating rules of decorum, taste and other factors, according to Rob Beschizza, the managing editor. The migration to the Disqus system for comments on the site preserves all of this. It also makes life easier for prolific commenters, as it allows the option to use a single identity across many Disqus-using sites. As an added bonus, it dramatically reduces the load of spam and slashes the time required to delete any that does get through. Mr Beschizza distinguishes anonymity, where no user information is required, and pseudonymity, in which users adopt a nom de commentaire, but are still required to show a valid e-mail address. The address is not displayed, nor is it divulged by BoingBoing to third parties. Requiring users to disclose it does, however, provide the first line of defence against automated spamming systems. It also puts off lazier discussants. But then, as Mr Beschizza notes, "very little useful commentary came in from unregistered pseudo-anonymous postings." (In keeping with BoingBoing's consensus-seeking spirit, Mr Beschizza emphasises that like all other staff, he does not speak for the site as a whole, and can only present his own views.) Most commenting systems that promise anonymity fail to deliver it. Anonymous accounts are still tracked in web logs and leave traces of activity across a site. BoingBoing is perfectly happy to allow users to employ any name they choose. It is left to the users to register via an avowedly anonymous mail service, like Hushmail, or employ a system like Tor to prevent tracking individual page requests. "If someone wants to be anonymous, they have to consciously make themselves anonymous," Mr Beschizza says. BoingBoing's comment policy is not an unfettered, anarchic free-for-all, in which all parties coming to the site may espouse any views they see fit. In Mr Beschizza's words, "free speech isn't a right to be published by other people." What people want is not so much the ability to comment, but a venue where their speech is amplified, he explains.  The site uses various techniques to facilitate civil discussion (which does not preclude heated exchanges which may involve conventionally unpalatable ideas). Moderators have the power to delete a post or ban a user, or leave a post in place in "disemvoweled" form. Disemvoweling, a term that dates back to the late 1980s in Usenet user groups, involves removing all the vowels from words. The results remain readable, though only with some mental effort, so most casual readers might not bother. The site also strikes off trolls and griefers. The former are chronically disagreeable sorts that make it their goal to refute any point, no matter how valid; the latter use techniques that may include the equivalent of emotional torture to get a rise out of others. Those who complain most loudly about being suppressed lack the self-awareness to understand that they are not engaging in a manner likely to allow the back and forth which constitutes a community of ideas, says Mr Beschizza. It is often hard to convince such individuals that they are behaving in an untoward manner; referring them to their own posts liberally sprinkled with obscenities and directed personal hostility is of little use.The comment landscape is in a state of transition, Mr Beschizza believes. In the near future authors will switch to centralised commenting systems which help them manage their online presence. Facebook's comments feature for websites (used by The Economist Asks polls like this one) already has some aspects of this. Twitter, too, relies on the same general principle, allowing a person to comment (tersely) on anything and post a link to the relevant website. In such a system, a commenter on a BoingBoing post would have a  permanent copy of his remarks on his own account, whether or not  BoingBoing opts to delete or disemvowel the post. He says, "If  everything is published somewhere, then our decision to publish  something or not doesn't destroy the fruits of someone else's  labour—even if it's stupid, crap, offensive labour. They're welcome to  it elsewhere."Mr Beschizza approvingly cites an essay published in July by Anil Dash, the first employee of blog-software firm Six Apart, and who is currently involved in not-for-profit efforts to help governments and citizens talk effectively to one another. Mr Dash called on sites with communities and forums actively to police themselves, rather than allow the most egregious participants to set the tone. "If your website is full of assholes, it's your fault," he writes bluntly. "And if you have the power to fix it and don't do something about it, you're one of them."]]>  


Watch out, there's a plane about

tag:www.economist.com,21528146
2011-08-30T12:09:53+00:00
2011-08-30T12:09:53+00:00

   A new way for light aircraft to avoid flying into one another  

The Economist online
http://www.economist.com


    AIRLINERS and air-traffic-control centres are in the process of adopting a new navigation system, called Automatic Dependent Surveillance-Broadcast (ADS-B), which uses the satellite-based global positioning system to work out where an aircraft is. ADS-B is more accurate than the existing arrangement, which is based on radar and signals from radio beacons, and will supplement it. Among other things, this should make automatic collision-avoidance systems more reliable. The anti-collision equipment currently fitted to jets has already helped make mid-air encounters between airliners rare, but many light aircraft and helicopters are not fitted with such kit. On average there are 12 mid-air collisions between small aircraft in America every year, causing 19 deaths—and a lot more near misses.One reason is that existing automatic collision-avoidance systems are too expensive for most private pilots. The shift to ADS-B should change that. But the new arrangements also have to be reliable. If pilots keep getting unnecessary alerts, they will tend to ignore them. If an alert is not issued when it should be, however, the result can be a mid-air collision.John Hansman and his colleagues at the Massachusetts Institute of Technology (MIT) think they have a solution to that. Their proposal is to surround each light aircraft with two concentric, vertical cylinders of airspace that resemble virtual “hockey pucks”. The smaller, central cylinder is called the Collision Airspace Zone (CAZ). It envelops the aircraft tightly and always remains the same size. Anything entering this volume of air is likely to hit the aircraft. The larger cylinder, which surrounds the CAZ, is the Protected Airspace Zone (PAZ). It changes in size during the different phases of a flight, according to the risk of a collision. For instance, when a series of aircraft are coming into land their speeds are low and they are under constant radar surveillance, so the PAZ of each can be smaller than when the same planes are en route, perhaps across an ocean or desert where there is no ground-based radar coverage. The higher the closing speed of two aircraft heading towards each other, the bigger the PAZ required, because the pilot needs more time to react to a warning.The level of warning is determined by an algorithm the researchers are developing. This predicts where each aircraft in the vicinity will be 60 seconds or more into the future. The ability to do that is made possible by ADS-B, which transmits an aircraft’s position, altitude and speed once every second to anyone who cares to listen. A moderate alert would be given if the CAZ of one aircraft was on course to enter another’s PAZ. A high alert would happen if the two CAZs were on a collision course. The pilots would then follow set procedures to take evasive action.To calculate how this will work in practice, and the optimum sizes of the buffer zones, Dr Hansman’s team have been using historical flight data from the San Francisco area and computer models of air traffic developed at MIT. This allows them to feed in many different flight patterns and adjust the algorithm accordingly. The team are working with America’s Federal Aviation Administration (FAA) and Avidyne, a firm that makes flight instrumentation, to develop the system. Flight testing should begin early next year and once the bugs have been shaken out of it, it might be used to improve the accuracy of the anti-collision systems fitted on airliners as well as light aircraft. It will also be used to help set certification standards for 2020, when the FAA has decreed that all commercial planes, and also the tens of thousands of light aircraft and helicopters that fly around America, must be equipped with ADS-B if they want to pass through controlled airspace.Dr Hansman’s invisible pucks could help, too, with another problem that has been worrying the FAA: the growing use of unmanned aerial vehicles. Although such drones are, for the moment, confined mainly to military areas, civilian versions are being developed for tasks like border patrols, and search and rescue missions. This often means flying in remote areas with no radar coverage. Drones already use GPS data to know where they are, but if they were equipped with ADS-B and the necessary algorithms they would be better able to avoid both each other and those quaint aircraft that still have pilots sitting in the cockpits.]]>  


The world's cleanest tank

tag:www.economist.com,21526721
2011-08-26T16:58:40+00:00
2011-08-26T16:58:40+00:00

   Researchers at CERN are using the world's cleanest chamber and a beam of fake cosmic rays to see how real ones may help seed clouds  

The Economist online
http://www.economist.com


    Researchers at CERN are using the world's cleanest chamber and a beam of fake cosmic rays to see how real ones may help seed clouds Read the article here]]>  


Difference Engine: Reality dawns

tag:www.economist.com,21526663
2011-08-26T07:52:38+00:00
2011-08-26T07:52:38+00:00

   The iPad’s rivals are learning some harsh truths about the tablet market  

N.V. | LOS ANGELES
http://www.economist.com


LAST week’s bombshell announcement by Hewlett-Packard that it was hiving off its personal-computer business—and, in particular, would cease making tablet computers and mobile phones forthwith—was greeted with shock and horror, plus a 20% plunge in share price. Canny investors promptly snapped up the depressed stock, realising it was the smartest move HP has made in years. More than anything else, the announcement showed that the firm had finally seen the light about the tablet market—namely, that there is no such thing.What exists instead is a rip-roaring market for iPads. Tablets based on Google’s Android, Hewlett-Packard’s webOS, Microsoft’s Windows, and Research In Motion’s BlackBerry operating systems have failed dismally to capture consumers’ hearts and minds the way Apple has with its iconic iPad. You only have to look at the numbers. Apple’s share of the tablet market is over 61% and growing, while all the Android tablets together make up barely 30% and are being squeezed. According to Strategy Analytics of Newton, Massachusetts, Windows tablets account for 4.6% and Research in Motion’s 3.3%. Sooner or later, the rest of the iPad wannabes are going to realise that, just because Apple has a runaway success on its hands, they cannot charge Apple prices for their hastily developed me-too products and expect consumers to clamour for them. It is not that Android tablets are technically inferior. Many more than match the iPad’s specification—though none feels quite as slim and svelte to the touch or as pleasing to the eye. Nor do any of the pretenders work as instantly and instinctively when taken out of the box. Add the classy consumer experience offered by Apple Stores, and the iPad’s sales proposition becomes irresistible. But the ultimate killer feature that Android and other tablets have failed to replicate is the care Apple took from the start to ensure enough iPhone applications were available that took full advantage of the iPad’s 9.7-inch screen. Today, over 90,000 of the 475,000 applications available online from Apple’s App Store fully exploit the much larger screen size. By contrast, only a paltry 300 or so of the nearly 300,000 apps for Android phones have been fully optimised for the Honeycomb version of the Android operating system developed for tablets—though many of the rest scale up with varying degrees of success.Overall, the difference between Apple and the rest is that, with the iPad (as with the iPod and iPhone before it), Apple invented a whole new product category—one that seamlessly integrates the company’s own hardware with its own means of delivering applications and content. All that tablet-makers like Acer, Asustek, Lenovo, Hewlett-Packard, Research In Motion, Samsung and Toshiba did was to squeeze a netbook computer into a thinner case by dispensing with the cover, keyboard and hard-drive. That made them, at best, suppliers of niche hardware. And yet, such is the hubris, they expect customers to pay Apple prices for their half-baked offerings.Take Hewlett-Packard’s now defunct TouchPad. This was priced initially at $499 for the basic 16 gigabyte version—the same starting price as the iPad. When there were few takers for the TouchPad because it was over-weight, under-developed and lacked key features like a rear-facing camera, the price was lowered to $399. And still the TouchPad failed to kindle interest among consumers. But when, last week, HP slashed the price to $99 to liquidate its unsold stock as it quit the business, TouchPads flew off the shelves faster than iPads have ever done. By some reckoning, three months supply disappeared in a day.What this sorry episode makes incandescently clear is that the price point for basic iPad wannabees is somewhat more than $99 but a good deal less than $399. When better equipped (though bulkier) netbooks can be had for $250, tablet-makers need to set their sights below $200. There is just one problem: the cost of the components currently used comes to more than that. According to the market research firm iSuppli, the basic TouchPad cost Hewlett-Packard $306 to build.Put that down to HP’s higher costs and lower volumes. By contrast, Apple designs its own iPad processors, its own software, its own batteries and its own enclosures—and has huge advantages of scale when sourcing its components from China and elsewhere. With a retail price of $499, the basic iPad 2 is believed to cost around $265 to build. So, how come Barnes & Noble can sell its Nook Color—an Android tablet masquerading as an e-book reader—for $249? Two factors are at work here. One is that the Nook Color has a more limited specification than most Android tablets on the market today. For instance, it has no front- and rear-facing cameras, no gyroscope, no GPS, no G3 wireless connection (only WiFi). It also has less internal memory, a seven-inch touch screen instead of the more usual ten-inch one, and relies on an older, less demanding version (Eclair) of the Android operating system. Skimping on components has probably saved $50 or more. The other reason is that Barnes & Noble is more interested in selling books and magazines than making fat profits from gizmos. The Nook is strategically important to the company’s future, having helped it grab more than 20% of the burgeoning e-book business in less than two years. Selling only through its own stores and online, the Nook’s retail markup is thought to be significantly less than the 50% on tablets generally.Interestingly, the Nook Color also provides clues to what Android tablet-makers will have to do if they are to survive and thrive. The device has no more bells and whistles than necessary, and it does the job it was designed to do extremely well. The screen, in particular, is exemplary. Barnes & Noble has promised to unleash more of the Nook’s hidden smarts in future releases. Meanwhile, hackers have embraced the Nook, “rooting” its underlying Linux software (equivalent to “jail-breaking” an iPhone) so it can run many more applications from Google’s online app store and elsewhere. Plug-in memory cards loaded with the necessary software are now available on the web for $35 and up, Installed in seconds, these let the device boot either as a Barnes & Noble e-book reader or as a cheap and powerful Android tablet. Some owners are rooting their Nooks so they can use them as Kindle readers as well. Such developments have not gone unnoticed at Amazon.com. The Kindle-maker is expected to release a “game-changing” tablet this autumn, featuring the latest Android (Honeycomb) operating system with a seven-inch touch screen and a price widely rumoured to be under $300. While the actual specification has yet to be made public, the new Amazon tablet will doubtless offer all the features, and more, of a rooted Nook.Given the popularity of Amazon’s existing Kindle, analysts believe the new device will quickly outsell all other Android tablets on the market, including Samsung’s Galaxy Tab. And as the world’s biggest online retailer, delivering downloads of movies, music and games as well as books, Amazon will be in a strong position to challenge Apple’s awesome combination of iPad, iTunes and App Store.Amazon has already shown that it can beat both Apple and Google to the punch by offering to store customers’ music collections in "lockers" in the cloud. Users can then access their tunes from any computer or Android device while on the move. Amazon’s “Cloud Drive” provides users with five gigabytes of storage for free. All together, that sounds like a pretty nifty way of building an Amazon-based ecosystem—and locking customers into it. Steve Jobs, Apple's charismatic former boss and architect of its remarkable self-supporting ecosystem, should be more than a little concerned.]]>  


Running foul?

tag:www.economist.com,21526765
2011-08-25T15:42:37+00:00
2011-08-25T15:42:37+00:00

   Oscar Pistorius's phenomenal performance is not all down to his high-tech artificial limbs  

C.S. | NEW YORK
http://www.economist.com


THIS weekend South African double amputee Oscar Pistorius will set his carbon-fibre prostheses into the starting blocks alongside able-bodied sprinters at the World Athletics Championships which begin on August 27th in Daegu, South Korea. The 24-year-old Mr Pistorius holds the double-amputee world records for all the sprint distances (100, 200 and 400 metres) and has been competing against non-handicapped athletes in international races since 2008. Last month he ran the 400 metres in 45.07 seconds, quick enough to qualify for Daegu, as well as for the 2012 Olympics in London.Born without the fibula, one of two bones which support the calf muscle, Mr Pistorius’s legs were amputated below the knee before his first birthday, the age by which most toddlers have learned to stand and many are learning to topple forward into their first steps. Incredibly, the simple physics of this tipping motion combined with his carbon-fibre calves have converged to produce one of the most efficient runners in history. Mr Pistorius began sprinting in January 2004 after sustaining a knee injury while playing rugby. Eight months later, aged 17, he won gold in the 200 metres at the Athens Paralympics, setting a world record in the process. His remarkable running economy was recognised by the International Association of Athletics Federations (IAAF), the sport's governing body. In March 2007 it intervened to prevent Mr Pistorius from racing against able-bodied athletes, introducing a rule banning devices incorporating springs. He appealed, submitting to tests comparing his gait and physiology to those of other athletes, to no avail; the IAAF upheld its decision. A year later, however, the Court of Arbitration for Sport, a tribunal deal with all manner of sporting controversies, overturned the ban, rebuking the IAAF for its handling of the matter.Much of the debate has centred around whether an amputee, with less muscle mass, has a metabolic advantage over those with their limbs intact. The rub is that measurements of an individual's metabolic capacity vary over time and are only ever indicators of potential performance. The highest aerobic capacity in a field of athletes is no guarantee of victory. Pistorius was found to be exerting 25% less energy than able-bodied athletes (a discrepancy he has no doubt been training to remedy ever since).There is less disagreement about Mr Pistorius’s unique stride. Sprinters typically reach their maximum speed somewhere between 60 and 80 metres and then strive to maintain it for as long as possible. Michael Johnson, who holds the world record for 400 metres, was admired for an uncanny ability to keep that pace up until the finish line. For his part, early in his career Mr Pistorius was the only athlete to run the 400 metres with faster split times in the second half of the race than the first (he now appears to have closed the gap).A runner’s stride can be divided into time spent with the foot in contact with the ground and time spent airborne, which is when the limbs are repositioned. Laymen might assume that elite sprinters are those capable of minimising foot contact and maximising air time. Peter Weyand of Southern Methodist University, however, has proven that in this respect there is no discernible difference between serious amateur sprinters and the Olympic elite. At peak velocity, top athletes spend under one-tenth of a second with one shoe on the ground; lower-tier runners take just a little longer. Remarkably, though, all runners spend 0.12 seconds in the air between steps. What distinguishes them is their ability to cover a distance in these intervals. As his prostheses warps and compresses under both his body weight and the considerable force exerted from his hips, Mr Pistorius's ground contact time is 0.11 seconds, slower than his elite counterparts. Surprisingly, the time he takes to reposition his limbs while airborne is a fleeting 0.09 seconds, or fully a quarter less than other runners. The only athlete to come close to this cadence is American Olympic bronze medalist Walter Dix who, at 175cm (5’9”), is considered short for an elite sprinter. This means Mr Dix's limbs have less distance to travel between steps, but also that he needs to cover a greater distance with each stride. By contrast, Mr Pistorius stands at 185cm with his prostheses on. That is a fair bit shorter than Usain Bolt, the world's fastest man, who is 196cm tall. Dr Weyand contends that Mr Pistorius's astonishing quickness is due to the light weight of the prostheses compared to the weight of a human lower leg.A casual observer might think that this offers less of an advantage than the apparent rebounding effect of a springy artificial limb. However, Nicholas Romanov, who both studies human biomechanics and coaches runners, does not think that the carbon fibre's elasticity is that much of a boon. Mr Pistorius's secret weapon is his ability, shared by only a handful of other athletes, including Mr Johnson and Carl Lewis, to use so-called gravitational torque throughout the race. This involves leaning forward and remaining constantly on the precarious tipping point between falling to the ground and maintaining controlled forward momentum with each step. All runners use gravitational torque when accelerating. Yet only the very best are able to maintain this precarious balance over the entire sprint distance.Rail-cam footage shows Mr Pistorius clearly bobbing up and down much less than most of his competitors, indicating his mastery of the technique. Few able-bodied athletes can replicate this astonishingly smooth stride. But the fact that some can, and that no other prosthesis user seems to have managed to emulate the South African runner, suggests that Mr Pistorius's success is not all down to high-tech limbs. Issues of fairness in sport have always been fraught, and are becoming ever more so as technology progresses. If Mr Pistorius is to succeed on the road to London, he will need not just talent, but tenacity, on the track and, most likely, in the courtroom. If his career so far is any guide, though, he has plenty of both.]]>  


Brainier chips

tag:www.economist.com,21526764
2011-08-24T14:18:00+00:00
2011-08-24T14:18:00+00:00

   A trio of surprising announcements at HP, IBM's new brain-like chip, and Skype gets into group messaging  

The Economist online
http://www.economist.com


    A TRIO of surprising announcements at HP, IBM's new brain-like chip, and Skype gets into group messaging.]]>  


Tablets reverting to type

tag:www.economist.com,21526759
2011-08-24T12:56:11+00:00
2011-08-24T12:56:11+00:00

   A new touch-screen keyboard may make touch typing less of a trudge  

P.M.
http://www.economist.com


    THE keyboards that appear on tablet computers using a touch screen,   such as the Apple iPad, can be a touch fiddly to use. They can be particularly frustrating for those who   have mastered the art of touch typing: resting your fingers on any part of the virtual keyboard,   as you would on a mechanical one, results in activating an unwanted key. And since the keypad is flat, there are no tactile hints as to where the keys are located.   As a result, typing can be slow and error prone. Now, though, Christian Sax and   Hannes Lau of the University of Technology in Sydney think they have   come up with an alternative. They call it the LiquidKeyboard and they unveiled its prototype on August 23rd at the Tech23 conference in Sydney.[inline|iid=1300]When the user puts his fingers on the surface of the screen a group of   keys morphs around each individual finger. The system senses the positions of the fingers and, by calculating the surface area of a   finger touching the screen, its relative pressure. The positions of   the surrounding keys are set in relation to each finger. The position of the groups of keys can be varied according to finger positions and a user's preferences. (See the video for a demonstration of how it works.) The system allows a typist to find keys and to touch type without tactile feedback, say the researchers, who are looking for partnerships to develop the technology. It might take a bit of getting used to, but for frustrated touch typists that might be worth the effort.]]>  


A multilayered solution

tag:www.economist.com,21526662
2011-08-24T09:56:43+00:00
2011-08-24T09:56:43+00:00

   Better e-reader screens might be made from onionlike ink droplets  

The Economist online
http://www.economist.com


E-READERS, such as Amazon’s Kindle, have been a commercial success. They have not, however, revolutionised the publishing industry in quite the way that many predicted they would. In part, that is because their displays are black and white, and they seem to many readers to be slow, grainy and—if truth be told—a little archaic. Better screens might make the difference between e-readers being intriguing gadgets and killer apps, and Shin-Hyun Kim and David Weitz, who work at the Experimental Soft Condensed Matter group at Harvard University, think they may have found a way to build those better screens.Unlike conventional display screens, which are lit from behind, e-readers use reflected light in a way similar to paper. Letters and other characters on the screen are formed out of ink that has a high optical contrast with the background, making them easy to read. The difference is that, rather than being printed into permanent shapes like normal ink, electronic ink is held in small capsules that can reveal it or hide it as required. The result is legible even in bright sunlight. But it often takes more than half a second to “turn” the page of an e-book (so displaying the 25 images a second needed for video is out of the question). And, although the size (roughly 100 microns across) of the elements, known as pixels, that make up the display is fine for monochrome reading, they would need to be a third of that or less to create sub-pixels of the three primaries (red, green and blue) that colour displays require. The answer proposed by Dr Kim and Dr Weitz, in a paper in Angewandte Chemie, is to change the way e-ink is manufactured.At the moment, such ink is composed of small, transparent spheres containing black and white particles suspended in a clear fluid. Half the particles are white and positively charged. The other half are black and negatively charged. When an electric field is applied, one lot is drawn towards it while the other is repelled. A negative charge attracts the positive particles, making the pixel appear black. A positive charge does the reverse.The problem, according to Dr Kim and Dr Weitz, is that the densities of the black and white particles are different, and therefore cannot both be made to match that of the fluid in which they are immersed. This slows down their movement, and thus the speed at which a screen can refresh its image. A better solution would be to immerse the black particles in one fluid and the white particles in another (so that in both cases their densities match that of the suspending liquid)—yet, at the same time, to continue to package both types of particle in a single sphere. To do so, the pair turned to a technology called microfluidics, which borrows from the techniques used to make computer chips to produce devices that mix small amounts of liquid in precise ways. Their own, particular device uses tiny channels to force two different liquids (one of which contains ink particles) in one direction down a channel, through a nozzle, thus bringing them into contact with two other streams travelling in the opposite direction. As the four streams collide they are forced into a third channel, forming layered droplets as they go. Normally, this sort of single-step mixing would not work, because of the difficulty of getting two liquids to flow stably through one channel. However, by using an oily liquid and an aqueous one, and by covering one side of the channel with a substance that repels water and the other side with one that attracts water, this can be avoided. The result is a “Russian-doll” droplet that, if the correct oily and aqueous liquids are chosen, can be made permanent by curing some of its layers into transparent polymers using ultraviolet light.To demonstrate, Dr Kim and Dr Weitz created what they call magnetic ink. This consists of an oily core containing magnetic particles mixed with carbon black, which is suspended within a watery layer that contains white polystyrene particles. That, in turn, is suspended in a transparent oily fluid. Like those in e-reader displays, the black and white particles can be drawn towards or away from the viewing surface (in this case using a magnetic field, rather than an electric one). They move much faster than those in traditional displays, though, because their densities are closer to those of the suspending media. If the new droplets can be incorporated into real screens, that will deal with the slow refresh rate.The next stage is to include all three primary colours in a single droplet. That is some way off. But if it proves possible, it will deal with the black-and-whiteness problem, too, by providing full-colour pixels that have the same number of droplets as monochrome ones.Turning this invention into a screen will take time. Indeed, it may never come to pass, for many other groups are approaching the e-reader problem from different directions. But whatever happens to this specific idea, Dr Kim’s and Dr Weitz’s invention is likely to have larger ramifications. It might, for example, be used to package together drugs in slow-release capsules of greater sophistication than is now possible.]]>  


Beware the cookie monster

tag:www.economist.com,21526571
2011-08-22T06:37:34+00:00
2011-08-22T06:37:34+00:00

   A virulently persistent form of user tracking has cropped up in the virtual wilderness  

G.F. | SEATTLE
http://www.economist.com


NO FEELING makes chills run more feverishly up and down a spine than the sense that its owner is being watched. People whose spines tingle a bit too often are typically branded as paranoid. Go on the web, however, and your spine ought reasonably to go into crisis mode. Internet users are indeed being tracked all the time. And shaking off a virtual tail can be tough. The reason is a tiny chunk of text called a cookie. It is inserted by websites into a browser to help them identify a user as he loads pages in succession, or returns on subsequent visits. They come with an expiration date, which can be minutes or years ahead, after which the browser deletes them.Cookies are a staple on sites which require users to log in, allowing for a continuous session after a single authentication. They may also be used to store preferences without an account, such as text-viewing size. Millions of sites use cookies for analytics; they permit a user's page requests to be divided into sessions and then aggregated into visitor counts and other metrics that tell a site's owners what people are reading, where they come from and how they move about. Cookies have a more directly commercial purpose, too. They are used to track behaviour, and so target advertising. Cookies let marketers pinpoint the sorts of offers likely to attract a particular user. That lets sites demand higher fees for virtual hoardings. Last October Babbage described the evercookie, a practical experiment by a programmer to demonstrate how persistent tracking codes might be embedded in a browser to follow users around, even when they made every effort to delete such tracking elements. The evercookie, designed by Samy Kamkar, used a grab bag of techniques to tuck away a unique identifier in non-obvious spots in a browser's cache, and packaged them together into a single chunk of programming code. To get rid of the evercookie, a user would have to delete the tag from every nook it was hiding in. Leaving even a single one would lead to the tag being multiplied and restored, or respawned, in the browser's cookie jar. Recently, a group of researchers released a report describing commercial use of two previously unseen types of tracking components, and the persistent use of an older technique. All three approaches enable indelible tracking. Worse, one of the methods follows a user even through a privacy mode where all pages, actions and caches are ostensibly deleted at the end of a browser session. Privacy modes are used on shared computers in libraries or hotels to prevent divulging previous users' private information, as well as for more lurid online activity which users would rather others (like parents or spouses) did not find out about.In 2009 the same group revealed that Adobe's Flash multimedia plug-in could store identifiers independently of the browser and that many websites would reinsert—or, more colourfully, respawn—a deleted browser cookie by consulting the Flash cache. Their new report examines these techniques. More cheerily, it also notes a substantial reduction in privacy-invading behaviour by the 100 most frequently visited websites. Ashkan Soltani, one of its authors, has just posted more extensive technical details to back up the report's conclusions. Flash storage of cookies persists, the report shows, but respawning has all but disappeared. Of Quantcast's top 100 sites, it was only found on the sites of Fox News and Hulu. That is an improvement from the six discovered by researchers in 2009; dozens more major portals outside the top 100 used the same third-party tracking firm. (Unrelated to this report, a researcher at Stanford said a few days ago that Microsoft's MSN.com portal was also respawning cookies; Microsoft immediately disabled what it said was outdated code.)  Hulu got the most flak. One of its ruses employed the ETag, a randomly generated snippet of code assigned by a web server uniquely to each item, like a file, it sends to a browser. Whenever the item is modified on the server—as when the text on a page is updated or an image replaced—its ETag changes. When a user requests the item from the server, the browser can send the ETag stored in its cache back to the server to see whether it matches the latest iteration. If it does, the requested item has not changed since the last time it was retrieved. The browser can then load the unchanged item directly from the cache without needing to download it again. This saves the user time and bandwidth. Every user should receive the same ETag for each item on a page, too.In Hulu's case, however, ETags were tinkered with. Instead of each user receiving a generic ETag, one object on a web page—a single icon, say—had an ETag that was in fact just the the user's unique browser cookie in disguise. As a result, the browser would, on visiting Hulu's site, send the doctored ETag, and with it the user's identity, back to the server. Deleting conspicuous cookies from the browser's cache would be no use, since browsers do not treat ETags as cookies. Private modes, too, are helpless against this stratagem because, unlike other records, a file's ETag is not deleted at the end of a private-browsing session. The only way to get rid of it is to empty the entire cache, which means having to download everything on the site from scratch on future visits. Another trick unearthed on Hulu's site relies on a feature of HTML5, the latest standard for how sites deliver pages and content to be displayed by a browser. HTML5 lets a browser store information in databases on the same computer on which the browser runs. This allows, among other things, offline reading of Amazon Kindle books or retaining information used by web apps without requiring a round-trip to a remote server, which can actually enhance privacy and security. However, such databases also turn out to be a perfect place to hide a user's virtual ID, which can then be respawned in the browser. On publication of the report, Hulu immediately discontinued using KISSmetrics, the firm to which it contracted some of its user tracking. Two other firms, Spotify and Gigaom, which the report noted sharing identifiers with Hulu, followed suit. For its part, KISSmetrics assures that identifiers were not exchanged among unrelated sites. All the same, the firm quickly changed its code, ditching persistent-tracking and respawning methods. The desire to track users continues to outstrip privacy features in browsers. Perhaps one day the cookie equivalent of anti-virus software will emerge. Until then, the more watchdogs sniffing around for intruders in the virtual marketplace, the better. ]]>  


Difference Engine: Devil in the details

tag:www.economist.com,21526237
2011-08-19T07:45:11+00:00
2011-08-19T07:45:11+00:00

   Choosing a big-screen television has become harder than ever  

N.V. | LOS ANGELES
http://www.economist.com


IF YOU have not gone shopping for a new television set for quite a while, enough has changed to require some serious thought. Your correspondent has finally given in to family pressure to create a dedicated media lounge. Given the limited resources, this is unlikely to be some 24-seat viewing room with a silver screen, curtains and digital projectors to rival the home theatres created for the likes of Steven Spielberg or Larry Ellison. The good news is that, with modern television sets, it does not have to be. A spare room, with a couch and a couple of easy chairs, plus a large enough flat-panel television and a reasonable audio system, can more than meet most family’s viewing needs. Before splurging on a fancy new high-definition television (HDTV) set, though, it is worth considering what features make sense and what do not. Start with the viewing angle. THX, a technical standards-setter for the video and audio industries, requires the back row of seats in a home theatre to have at least a 26º viewing angle from one edge of the screen to the other. Seats nearest the screen should have a viewing angle of no more than 36º. These subtended angles correspond to a viewing distance of roughly 2.2 times the screen width at the back row of the seating down to 1.5 times the screen width at the front. Within these limits, viewers should be able to enjoy the most immersive experience. The question then is how to relate viewing distance to a person’s visual acuity. In other words, what is the maximum distance beyond which some picture detail is lost because of the eye’s limitations? Visual acuity indicates the angular size of the smallest detail a person’s visual system can resolve. This depends on the sharpness of the retinal focus within the eye, and the sensitivity of that part of the cortex that interprets visual stimuli. Someone with 20/20 vision (6/6 in metric terms) can resolve a spatial pattern (of, say, a letter in the alphabet) where each element within it subtends an angle of one minute of arc when viewed from a distance of 20 feet (six metres). In other words, a person with 20/20 sight should, in normal lighting conditions, be able to identify two points that are 0.07 of an inch (1.77mm) apart from a distance of 20 feet. Twenty feet is taken because, as far as the eye is concerned, it is effectively infinity. A person who can detect individual elements that make up, say, the letter “E” on the eighth line of an optometrist’s Snellen chart—and thereby recognise that the letter is an “E” and not a “D”—is said to have normal 20/20 eye sight. Someone with 20/40 sight can see objects at 20 feet that those with normal sight can see from 40 feet. In many countries, 20/200 is the legal definition of blindness. Meanwhile, 20/20 vision is not perfect vision; it is merely the lower limit of normal sight. The maximum acuity of the human eye is around 20/8. Some birds of prey are thought to have eye sight as sharp as 20/2.As far as watching television is concerned, visual acuity represents the point beyond which some of the detail in the picture can no longer be resolved by the conical receptor cells in the retina of the eye. It will simply blend into the background instead of being seen as a distinct feature. Thus, it is a waste to make individual pixels—the tiniest elements in a display—appear smaller than 0.07 of an inch when viewed from 20 feet.The problem with viewing images on a television screen—especially a high-definition one like the 1080p HDTV sets in use today—is that most people sit too far back. A survey made some years ago by Bernard Lechner, a television engineer at the former RCA Laboratories, near Princeton, New Jersey, showed that the median eye-to-screen distance in American homes was nine feet. At that distance, a 1080p HDTV set (with a screen 1,920 pixels wide and 1,080 pixels high) needs to be at least 69-inch across a diagonal if viewers are to see all the detail it offers. In practice, the most popular television size in America today is 32 inches. To see all the detail on a 1080p set of that size means dragging the chair forward from nine feet to a little over four feet from the screen. If it were an older 720p television set (1,280 pixels wide and 720 pixels high), sitting six feet from the screen would suffice to see the full quality of the image.Put another way, viewers cannot enjoy the full benefits of the higher pixel count of 1080p television if they sit any further back than 1.8 times the screen width. At 2.7 times the screen width, they might as well use a cheaper 720p set instead, as the eye cannot resolve the finer detail offered by a 1080p screen at that distance. Unfortunately, while 720p sets offer good value, they are becoming difficult to find. Manufacturers focus all their marketing efforts these days on higher-margin 1080p sets.As far as screen sizes and viewing distances are concerned, a room measuring ten feet by 12 feet is therefore more than adequate for watching a 50-inch television set, with viewers no further than six-and-a-half feet from the screen. The question, then, is what kind of 1080p set to use—plasma display, liquid-crystal display (LCD) or the latest light-emitting diode (LED) variety? Plasmas, with their rapid switching and deep blacks, have long been the favourite for sports fans and movie buffs. Apart from their lack of blur and judder when tracking fast-moving objects and their freedom from wishy-washy greys, they can be viewed from wider angles than LCDs without the picture changing colour. They also produce better three-dimensional images, primarily because they generate less ghosting (double images) when using 3D glasses. But plasmas have lately fallen out of favour because of their bulk and thirst for power. More to the point, manufacturers have begun to fix many of the LCD’s faults. To lick the motion problem, LCD set-makers have developed special circuitry for estimating and compensating for any rapid movement within a scene. This increases the screen’s frame rate from the 60 hertz of traditional television to 120 hertz and even 240 hertz. A few manufacturers have begun offering sets with refresh rates of up to 480 hertz, with 960 hertz on the horizon.Unfortunately, the motion-compensating circuitry can make filmed content appear like a cheap video—a glitch known in the trade as the “soap-opera effect”. The source of the problem is the way film shot at 24 frames a second has to adjust to television's refresh rate of 60, 120 or even 240 frames a second. One way of doing this is to analyse first one frame of film and then the next, and calculate an average of the two. This interpolated frame is inserted between the first and second frames, and the process repeated for each successive frame of the film. The interpolation process is good at removing blur and judder, but it can make the motion appear unnaturally smooth and disconcerting. Be warned, 240-hertz sets are the worst offenders.Lastly, there are the LED sets. Manufacturers would have you believe these are a new form of display. They are not. They are simply LCD televisions that use LEDs for backlighting instead of the usual fluorescent tubes. The LEDs can be either along the edges of the screen or spread as an array behind the whole of the display. Edge-lit displays have problems with uniformity of brightness as well as a limited viewing angle. Screens that use a full-array of LED backlights are much better. Apart from giving more uniform brightness, they allow the screen to be dimmed selectively in places where a scene needs to be dark. The effect is to make the LCD’s blacks appear almost as dense as a plasma’s. Only top-of-the-range LCD sets from Sharp and Sony currently have this feature. Expect to pay dearly for it.So, what to choose? That depends on budget and personal preferences. All things being equal, plasma televisions are about two-thirds the price of their LCD equivalents, which are themselves up to a third cheaper than LED sets. Meanwhile, the premium that 3D sets once commanded has all but vanished. They are now worth buying, not so much for their ability to show 3D content, but because they display 2D even better than conventional plasma or LCD sets (see “Beyond HDTV”, July 28th 2011).As a sports-loving, old-movie addict, your correspondent’s choice is easy. With the help of a brother-in-law in the business, he has ordered a Panasonic Viera TC-R50VT20, a 50-inch plasma set with all the bells and whistles (arigato, Hiroshi-san). He recommends others read the annual ratings for television sets published in Consumer Reports (March 2011), then go to the nearest big-box store and see for themselves. One rule of thumb: manufacturers’ recommended prices average around $36/inch for plasma televisions and $48/inch for LCDs. Discounts in-store and online should lower such prices by at least 20%. Do not settle for less.]]>  


Out of Africa

tag:www.economist.com,21526040
2011-08-18T17:12:53+00:00
2011-08-18T17:12:53+00:00

   How African is the latest tablet from Congo?  

J.O. | PARIS
http://www.economist.com


J.O. appears courtesy of Global Voices Online, an international community of bloggersAFRICAN technology pundits cheered when a Congolese company announced in June that it would be launching a new Android tablet computer "designed in Congo" in September. Technological innovation is rare on the continent and advocates and consumers alike latch onto every new development, often regardless of quality or price. When I raised serious doubts about whether the new tablet could truthfully be called African, several Africans refused to consider the possibility, accusing me on Twitter (this and most other links in French) of being a traitor to the cause of African development or, hailing from Cameroon, envious of go-ahead Congo.More than two weeks since an article published on Global Voices raising questions about whether the tablet was not in fact Chinese, no official statement from the company or the device's claimed inventor, Verone Mankou, except for a message from Mr Mankou on Twitter promising to write something on his blog. Nothing has appeared so far.Mr Mankou is the 25-year-old CEO of VMK, a company that was until recently devoted to web design. He is also an IT advisor to Thierry Moungalla, the Congolese minister whose brief includes postal services, telecommunications and new communication technologies. Mr Moungalla is hosting the Africa Web Summit in Brazzaville where VMK plans to unveil the tablet on September 17th. According to the summit schedule Mr Moungalla will present the opening statement immediately before the VMK tablet is presented to the audience.The VMK tablet will sell for 200,000 Central African francs (around $400), or four times Congo's monthly minimum wage. Even so, many people may wish to purchase it solely to support African innovation. At first, most observers welcomed the news of VMK's product and praised the idea publicly as a revolution for the continent. This excitement soon faded, however, when it emerged that a device with very similar specifications can be purchased online from China for less than half the price. And, if you buy in bulk, the producer will, for a small fee, print a logo of your choice on each gadget.Mr Mankou does not deny that his VMK tablet will be manufactured in China, and even published photos from the factory in Shenzhen on his company blog. The question is, therefore, whether there is in fact anything African at all about this product. As Mr Mankou himself admits, he delegated the design of some important features to a friend in Canada and partners in Asia. No one has yet tested the tablet or seen anything other than photos of a prototype. His investors remain anonymous.None of this is untoward in its own right. Mr Mankou avers that the VMK tablet does not resemble anything already on the market. He also claims that VMK will provide after-sales services as part of the price, though he does not provide any details of how that might work. And some of his comments suggest that the tablet's software may well have needed tweaking for the Central African market. If true, this could go some way to explaining the price gap. Still, consumers deserve to know what in the VMK device makes it worth twice as much as the non-African version. So far, they have only Mr Mankou's word that "VMK would not invest $160,000 to put a logo on a gadget that already exists."Zimbabwe and Nigeria have previously seen high-profile cases of Africa-branded technology that turned out to be Chinese. There is nothing wrong with Chinese technology, of course. It is cheap and often perfectly decent, making it the ideal choice for many cash-strapped African consumers. In the short term, what matters is that Africans can get their hands on affordable high-tech goods; these needn't necessarily be African. But ultimately, these will not be a replacement for home-grown innovation.]]>  


Living on the EDGE

tag:www.economist.com,21526201
2011-08-18T12:08:53+00:00
2011-08-18T12:08:53+00:00

   The world's most popular mobile data standard has been cracked  

G.F. | SEATTLE
http://www.economist.com


MOBILE operators like to trumpet the speed of their fastest networks. Third-generation (3G) service is now old hat, and 4G networks—whether legitimately labeled as such or not—are the latest fad. Meanwhile, slow-and-steady 2G GSM-based service quietly remains the most widely used cell technology worldwide. That is a problem, according to Karsten Nohl, a member of a research team that has cracked the encryption protocol used for most of the data sent and received around the globe. (The researchers have a technical presentation available for download.) This decryption effort enables outsiders to eavesdrop on data connections or voice calls placed over a 2G network in a jiffy. Dr Nohl says that his team's test laptop, a reasonably powerful modern machine, may crack a call in 11 minutes using just €10 ($14) in radio components. The crack must be repeated for every connection, however; it does not allow unfettered and continuous access to all conversations and information sent on 2G networks. But it does make it possible for specific sessions to be intercepted, making the method useful in targeting particular activities, businesses or individuals. Dr Nohl stresses that the 11 minutes was just a first pass at writing the cracking software, and that his group used only modest equipment with no financial motive. Criminals, by contrast, could benefit mightily from accelerating the crack, he says, one reason his group has refrained from expounding the technique in detail. It has, however, pointed to some specific holes which ought to be plugged. The group found some networks disabled all security features, relying on the highly misguided notion that traffic could not be easily intercepted except by mobile operators. Having no security from the phone to a base station on a mast makes it easier to filter and monitor traffic. In 2009 Dr Nohl and colleagues pointed out significant weaknesses to the base GSM standard. Their new attack focuses on General Packet Radio Service, better known as GPRS—a modest improvement to GSM—introduced commercially in 2000. GPRS allows rates of tens of kilobits per second (Kbps), while a subsequent tweak known as EDGE allows downstream rates of 200 to 400 Kbps. GPRS and EDGE are commonly referred to as 2.5G, sitting in between 2G and 3G network speeds. Over 5 billion GSM mobile subscriptions are active worldwide, but just over 700m of those have access to 3G service, according to Informa Telecoms and Media, a research firm. And 3G service is an add-on to existing 2G networks, not a fully fledged replacement. A 3G phone can conserve energy and free up 3G spectrum for internet use by switching to 2G for voice calls and slow data connections. GPRS and EDGE remain dominant outside metropolitan areas and in developing countries. Several hundred million subscriptions are for all-important machine-to-machine connections, such as ATMs authenticating bank transactions, smart-meter reporting, vending machines sending automated orders for more cans and bottles, or remote alarm monitoring. Nokia Siemens Networks, a maker of communications kit, estimates that this figure will rise to one billion by 2015. "It really is the information backbone of what people call the smart society," says Dr Nohl. The industry's trade group has taken previous disclosures seriously. Operators can take measures to upgrade security on some phones. But network-wide upgrades will do little to keep basic phones and smartphones secure. Fortunately, GPRS's security flaws can be fixed by using encryption enabled in software that runs on individual phones, either through apps or the device's operating system. Apps could, for instance, enable encrypted web browser sessions, which have become available on desktop browsers as an option for all Facebook and Twitter traffic, as well as being an absolute requirement for some time on e-commerce, banking and investment sites. In fact, many smartphone apps may already silently employ encryption to protect data over both mobile and Wi-Fi networks. Surprisingly, app makers have so far been loth to trumpet such measures, even though the cost of such security is low in most cases. Additional encryption would not prevent perpetrators from penetrating GPRS connections. But such intrusions would net only scrambled bits. ATM and other machine-to-machine networks already tend to use encryption anyway, but Dr Nohl notes that some companies that run them may put too much faith in the mobile network's security. This is becoming increasingly unwise. The silver lining is that 3G networks boast far superior security protocols. Dr Nohl's team has examined these faster and more sophisticated networks, too. So far, they have found no glaring flaws. 3G networks rely on heavily scrutinised algorithms which are publicly available so anyone who spots a potential problem can signal it immediately. That said, 3G will not replace 2G networks in most countries for well over a decade. Before that happens, additional cryptographic protection is the only safe bet.]]>  


A gamble for Google

tag:www.economist.com,21526313
2011-08-17T17:51:24+00:00
2011-08-17T17:51:24+00:00

The Economist online
http://www.economist.com


    Google buys Motorola Mobility, social networks face shutdowns and a new biography of Steve Jobs attracts speculation]]>  


Doing the maths

tag:www.economist.com,21526280
2011-08-17T12:47:49+00:00
2011-08-17T12:47:49+00:00

   Was the price Google paid for Motorola Mobility right?  

K.N.C. | TOKYO
http://www.economist.com


GOOGLE'S $12.5 billion acquisition of Motorola Mobility seems pricey. The sum amounts to a 63% premium on the ailing device maker's share price before the deal. The purchase is widely regarded as being about Motorola's patents, which Google needs to defend itself from a spate of recent lawsuits. How might Google, a company famously fond of numbers and maths, have arrived at the amount? The eagle-eyed industry analysts at Frost & Sullivan offer a plausible answer. In a commentary issued on August 16th they note:"Motorola has a portfolio of 24,500 patents and patent applications that instantly bolsters Google’s strength in the IP war. Looking at some recent patent auctions and using some simple math can show why these patents were indeed the target of Google’s acquisition.Using one of the industries recent patent auctions as a baseline, in December of 2010, Novell sold off its portfolio of 882 patents for $450 Million. A simple division calculation leads us to a value of $510,204.08 per patent. Why not round that figure off you ask? Well, let’s look at the patent value of the Motorola acquisition.Forgetting that Motorola also makes mobile phones, let’s say the entire value of the acquisition was in their 24,500 patents and applications. At a $12.5 billion price tag, that equates to…drum roll please…$510,204.08 per patent. Can anyone guess what heuristic they used in the board room in valuing the deal?In the Motorola acquisition, Google bought a patent portfolio and got a mobile phone business thrown in for free."If Google's acquisition of Motorola was indeed priced solely on a cost-per-patent basis, as looks likely, it would set a benchmark for valuing an intellectual property portfolio. It would also brighten the prospects of other firms that are considering selling their patents. For instance, Kodak, which holds more than 1,000 related to digital imaging, is keen. And it would justify a higher share price for Research in Motion (RIM), the maker of the BlackBerry. The Canadian company has been losing market share for years—but it is sitting on around 2,000 patents, with 3,000 more applications in the pipeline.Still, valuing intellectual property remains more art than science. In July six IT firms including Microsoft, RIM and Apple paid $4.5 billion for 6,000 patents owned by Nortel, a bankrupt Canadian manufacturer of telecommunications gear. Using the same simple maths, the group paid a clean $750,000 per patent. If Google's latest acquisition was pricey, that one was downright exorbitant.]]>  


Sweeping change

tag:www.economist.com,21526162
2011-08-17T11:33:08+00:00
2011-08-17T11:33:08+00:00

   A remote-controlled robot does away with an unenviable profession  

A.A.K. | MUMBAI
http://www.economist.com


ON SEPTEMBER 18th, 2005, a week after the fourth anniversary of the deadly September 11th attacks on the World Trade Centre in New York, Fahad Azad, a 23-year-old from India, was detained at Dubai airport. His metal briefcase had set off a security alarm during a routine baggage inspection. Mr Azad, an automobile-engineering student, must have seen this coming. The briefcase, a potpourri of electronic items included a gadget which had an uncanny resemblance to PackBot, a military robot used by American ground troops in Iraq and Afghanistan. In reality, it was a harmless device designed to sneak into hard-to-reach air-conditioning ducts and clean them. An amused security team at the airport let him off but not before a thorough (verbal) demonstration of how the device works. “They couldn’t believe that the robot was an Indian creation,” recalls Mr Azad who later christened the contraption DuctBot. After countless revisions, the 2.5kg unit now resembles a miniature Buick Bug from 1910. Mr Azad chose to mount the DuctBot on wheels rather than mechanical limbs because they offer more energy-efficient locomotion and are easier to steer. This is done using a wireless Sony PlayStation 2 (PS2) joystick over the 2.45GHz radio-frequency band used in remote-controlled toys. The PS2 joystick is much easier to use than industrial devices, which are also five times more expensive. (The robot also responds to Nintendo Wii’s motion-control interface, but the Wii has not yet found any takers. “People here find it funny to move their arms and legs to drive the robot,” explains Mr Azad.) The robot is designed to snake through dark, narrow air conditioning ducts and spot obstacles along the way. A pair of light-emitting diodes (LEDs) fitted in the front and at the back light up the grubby scenery so it can be captured by a camera lens. The images are transmitted to a monitor or a digital video recorder. On noticing an obtrusion the controller sets in motion a soft-bristled brush, or blows compressed air through tentacles attached to it. The robot can flush out many sacks of dirt, as well as dead pigeons, rodents and insects.Maintaining healthy indoor air quality and monitoring carbon-dioxide levels in buildings with central air conditioning is a challenge. It is critical in places where clean air can mean a difference between life and death, such as hospitals. Stale air can, for instance, cause post-operative complications like infection. To limit such risks modern air conditioners come with humidity and CO2 sensors which regulate room temperature based on the number of people occupying it. Inbuilt air filters help trap pollutants like dust, pollen, animal hair and mold. But if ducts are neglected the sheer quantity of unpleasant stuff in them can overwhelm even the fanciest systems.EPSCO, a Dubai-based company which specialises in improving indoor air quality, read about Mr Azad’s invention in a national newspaper after he had won an international robotics competition. EPSCO had the cleaning equipment but it needed someone to get into those ducts to do the dirty, dangerous work. Across India, for example, the unenviable task still falls to children small enough to squeeze through them. In a procedure eerily reminiscent of the dismal lot of Victorian London's child chimney sweeps, they strap on a mining torch and carry a broom. In 2005 Mr Azad, then still at university, decided to do something about it. Six years later his start-up, Robosoft Systems, has the Indian Navy and Blue Star, a leading air-con maker, as clients.Mr Azad is keen to branch out from the duct-cleaning business. His ten employees are currently exploring robot designs to tackle inspecting oil tanks for cracks or detecting leaks in sewage pipes. Their biggest challenge is to make the robots both robust and user-friendly enough that they could eventually be operated not by engineers but by labourers, often with almost no formal education and precious little experience of technology. Using a games console's intuitive control system is surely a step in the right direction.]]>  


Even unlimited access has its limits

tag:www.economist.com,21525853
2011-08-17T07:37:54+00:00
2011-08-17T07:37:54+00:00

   Unlimited broadband access is all too often not quite what it claims to be  

G.F. | SEATTLE
http://www.economist.com


HUMPTY DUMPTY told Alice that "When I use a word, it means just what I choose it to mean—neither more nor less." American telecommunications companies seem to have adopted a similar semantic strategy when they use the word "unlimited". AT&T is the latest to join the bandwagon. It has just imposed a cap on unlimited mobile data plans for the heaviest users, soon to be followed by limits, and fees for exceeding them, on its wired broadband network. The deployment of zippy next-generation 4G networks, too, is hampered by outdated caps on usage many telecoms firms have and will put in place.Verizon Wireless engaged in doublespeak of its own several years ago with its first-generation 3G offering, which the operator plugged as "Unlimited Broadband Access". In a footnote of its terms, however, Verizon explained that although e-mail, web browsing and remote corporate activities were not subject to limits, streaming and downloading any media was forbidden altogether. As a result, any use beyond 5GB per month risked engendering potential early termination of the contract on the assumption that even unlimited legitimate use would not exceed that figure. In 2007 the company reached an agreement with the state of New York to stop using the term "unlimited" and compensate users terminated for exceeding the five-gigabyte limit. It also paid a modest fine. Since Verizon was slapped down, cable and telecoms firms have become more sparing in their use of universal adjectives. At the same time, they have become cagier about explicitly revealing in what ways they restrict service. Comcast, for instance, appears to have had a secret limit for its cable broadband beyond which customers were cut off. Use more than 250GB a month for two months in one year, and your service could be cut. Comcast was eventually shamed into confessing to this ruse, even though it had never promised customers could download or stream endlessly. Undisclosed caps might prove a problem in the courts or with regulators, but limits on use, so long as they are imposed across the board and do not target particular types of data like videos or music, do not violate network-neutrality rules adopted by the Federal Communications Commission (FCC). Nor do they fall foul of the broader principles that underpin the FCC's decision. What irritates many users, other than being shut out from the network for no apparent reason, is the network operators' putative justification for imposing such limits in the first place. All manner of networks have peak periods when capacity is stretched. This might occur at particular times of day, or while political or sporting events are on. Using dynamic pricing or peak-usage thresholds to reduce usage is a perfectly sound idea. By contrast, rationing data, which disconnecting a bandwidth-hogging user amounts to, is an inefficient way to manage a scarce resource. AT&T's latest ploy is more elaborate. The company is targeting the heaviest 5% of subscribers to the unlimited plans it offered from 2007 to 2010. As a result up to a million users of the 20m or so with unlmiited plans, may be affected each month if the size of their online diet is deemed too grasping. Instead of cutting off service completely, however, AT&T has taken a leaf out of its European peers' rulebook and plans merely to throttle connection speeds available to the most egregious digital gluttons. These will probably be knocked onto 2.5G GPRS or EDGE networks (with speeds of roughly 50 kilobytes per second and up to 200 kilobytes per second, respectively) that AT&T will maintain for the foreseeable future. (Data received over Wi-Fi, whether at AT&T's hot spots or on private networks, does not count towards the totals.) Networks already employ throttling and congestion management in each cell in a network. A network can only push out so much data at any given time and operators expend substantial effort to ensure that no subscriber is entirely cut out by pinching bandwidth from extant connections, slowing them down. One could argue that AT&T is simply taking this a step further by extending it from instantaneous use to longer periods and across its entire network. These new limits, reasonable though they may be during peak times given current capacity, remain in place as more 4G networks, with much higher speeds and throughput, are rolled out. This happened with Comcast's cable broadband, which remains subject to a 250GB monthly cap even as the network's capacity increased tenfold in some areas, and at least doubled in most others, from 2008 levels. Verizon offers plans up to 10GB per month before extra fees kick in despite continually expanding its 4G network. This is equivalent to just two hours of full-throttle transfers—two or three high-definition films, say—or a few dozen hours of lower-resolution video streaming. Public Knowledge, an advocacy group which styles itself as a defender of digital consumer rights, recently released a report making the point that a blazing 4G network is useless if caps mean that all one can conceivably afford to use it for could be done equally well over existing 3G networks. As it stands, AT&T's plan means that 5% of users will have their mobile broadband crimped each month, regardless of future improvements to network capacity. Many may choose to pay up to restore speedier connections. Operators are right when they point out that caps and fines are perfectly legitimate ways to keep a network fluid. Their true intentions will, however, be revealed when the 4G floodgates open. If the restrictions are not loosened, they can no longer be justified as a reasonable tax on bandwidth hogs who make online life slow and miserable for more restrained types. They will start looking more and more like a rent-seeking ruse.]]>  

